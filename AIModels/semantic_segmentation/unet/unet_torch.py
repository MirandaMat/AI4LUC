# -*- coding: utf-8 -*-
"""DeepLabv3plus_normalized0to1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17qCHgOV7M-0_gzwE4RPJ2NC361KbiG5j
"""

# -------- Library --------
# Machine Learning
import torch
import torch.nn as nn
from torch import Tensor
import segmentation_models_pytorch as smp
import tensorflow as tf

# Data
import rasterio as rio
import csv
import pandas as pd
from mpl_toolkits.axes_grid1 import make_axes_locatable
from tifffile import imread
import numpy as np
import gc
import imghdr
from torchvision.datasets.vision import VisionDataset
from torch.utils.data import DataLoader
from torch.utils.data.dataset import Dataset
from torchvision import transforms

# Graph
from typing import Any, Callable, Optional
from tqdm import tqdm
import matplotlib.pyplot as plt

# Directory manager
import os
import glob
from copy import deepcopy
from pathlib import Path
import copy
import time

# Metrics
from sklearn.metrics import accuracy_score, f1_score, balanced_accuracy_score

# -------- Set up --------
# Classes
lista_class = {'building': 0, 'cultivated_area': 1, 'forest': 2, 'non_observed': 3, 'other_uses': 4,
               'pasture': 5, 'savanna_formation': 6, 'water': 7}

lista_class_r = {value: key for key, value in zip(lista_class.keys(), lista_class.values())}

# Constants
IMG_HEIGHT = 256
IMG_WIDTH = 256
NUM_CLASSES = 8
BATCH_SIZE = 64
EPOCHS = 80
PATIENCE = int(EPOCHS * (10 / 100))
exp_directory = '/scratch/ideeps/mateus.miranda/ai4luc/segment_models/deeplabv3plus/'

# Transform data to Tensor
# dataTransform = transforms.Compose([transforms.ToTensor()])

# GPU config
torch.backends.cudnn.determinstic = True
torch.backends.cudnn.benchmark = False

# Fetching the device that will be used throughout this notebook
device = torch.device("cpu") if not torch.cuda.is_available() else torch.device("cuda:0")
print("Using device", device)


# -------- Metrics --------
def compute_iou(label, pred):
    unique_labels = np.unique(label)
    num_unique_labels = len(unique_labels)
    iou = {}

    for index, val in enumerate(unique_labels):
        pred_i = pred == val
        label_i = label == val

        I = float((label_i & pred_i).sum())
        U = float((label_i | pred_i).sum())
        iou[lista_class_r[val]] = (I / U)

    return iou


# ------------- Load data -------------
# Load images
def load_patches(path):
    # Create the sequence of the images path sat[0]
    image_path = path[0] + "/images/**/*.tif"
    # Take full path
    image_names = glob.glob(image_path)
    # File sort
    image_names.sort()

    # Create the sequence of the mask path sat[1]
    label_path = path[1] + "/masks/**/*.tif"
    # Take full path
    label_names = glob.glob(label_path)
    # File sort
    label_names.sort()

    # lists
    image_patches = []
    images_normalized = []
    label_patches = []

    # Reading images path
    for name in image_names:
        with rio.open(name) as raster:
            bands = []
            for i in range(1, 4 + 1):
                bands.append(raster.read(i))
            image_patches.append(bands)

    # Saving as array
    image_patches = np.array(image_patches).astype('float32')

    # Reading labels path
    for name in label_names:
        with rio.open(name) as raster:
            # Read label
            label = raster.read(1)
            label_patches.append(label)
    # Saving as array
    label_patches = np.array(label_patches).astype('int32')

    # Normalizing 0 to 1
    s = tf.keras
    normalized_raster = tf.keras.layers.Rescaling(scale=1 / 255)(image_patches)
    image_patches = np.expand_dims(normalized_raster, axis=0)

    # Transform to tensor
    image_patches = torch.tensor(image_patches)
    label_patches = torch.tensor(label_patches)

    return image_patches, label_patches


# Load images and labels together in only on dataset
class SegmentationData(Dataset):
    def __init__(self, image_patches, label_patches):
        self.image_patches = image_patches
        self.label_patches = label_patches

    def __len__(self):
        return len(self.label_patches)

    def __getitem__(self, index):
        return self.image_patches[index], self.label_patches[index]


# --------------- Model ---------------
# U-Net
def unet_model(num_channel: int, num_classes: int):
    unet = smp.Unet(encoder_name='resnet101', encoder_weights=None,
                    in_channels=num_channel, classes=num_classes,
                    activation='softmax')

    return unet

# Training
def train_model(model, criterion, dataloaders, optimizer, metrics, save_path, num_epochs, name_dataset):
    global loss, batchsummary
    start = time.time()
    best_model_wts = deepcopy(model.state_dict())
    best_loss = 1e10

    # Use gpu if available
    model.to(device)

    # Initialize the log file for training and testing loss and metrics
    fieldnames = ['epoch', 'train_loss'] + \
                 [f'Train_{m}' for m in metrics.keys()]

    # Save the log in csv file
    with open(os.path.join(save_path, 'log_1_' + name_dataset + '_' + '_DeepLabv3plus' + '.csv'), 'w',
              newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writeheader()

    for epoch in range(1, num_epochs + 1):
        print('Epoch {}/{}'.format(epoch, num_epochs))
        print('-' * 10)

        batchsummary = {a: [np.nan] for a in fieldnames}
        batchsummary['Train_iou'] = {a: [np.nan] for a in lista_class_r.values()}

        model.train()

        for image_patch, label_patch in tqdm(iter(dataloaders)):
            # Normalizing data

            inputs = image_patch.to(device)
            masks = label_patch.to(device)

            # zero the parameter gradients
            optimizer.zero_grad()

            # Take the trained_models predictions
            outputs = model(inputs)

            # Create a valid target tensor for nn.CrossEntropyLoss
            masks = torch.squeeze(masks).type(torch.long)
            loss = criterion(outputs, masks)

            # Get class indices predicted
            y_pred = torch.argmax(outputs, dim=1).data.cpu().numpy().ravel()
            y_true = masks.data.cpu().numpy().ravel()

            for name, metric in metrics.items():
                if name == 'f1_score_weighted':
                    batchsummary['Train_' + name].append(
                        metric(y_true, y_pred, average='weighted'))
                elif name == 'iou':
                    iou = metric(y_true.astype('uint8'), y_pred)
                    for key, value in iou.items():
                        batchsummary['Train_' + name][key].append(value)
                else:
                    batchsummary['Train_' + name].append(
                        metric(y_true.astype('uint8'), y_pred))

            loss.backward()
            optimizer.step()
        batchsummary['epoch'] = epoch
        epoch_loss = loss
        batchsummary[f'train_loss'] = epoch_loss.item()
        print('Loss: {:.4}'.format(loss))

    model.eval()
    with open(os.path.join(save_path, 'log_1_' + name_dataset + '_' + '_DeepLabv3plus' + '.csv'), 'a',
              newline='') as csvfile:
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)
        writer.writerow(batchsummary)
    if loss < best_loss:
        best_loss = loss
        best_model_wts = deepcopy(model.state_dict())

    time_elapsed = time.time() - start
    print('Training complete in {:.0f}m {:.0f}s'.format(
        time_elapsed // 60, time_elapsed % 60))
    print('Lowest Loss: {:4f}'.format(best_loss))

    # Save the best trained_models  weights
    model.load_state_dict(best_model_wts)
    torch.save(model.state_dict(),
               os.path.join('./models_trained/deeplabv3plus/exec1/', name_dataset + '_' + "_DeepLabv3plus.pt"))


# Main
def main(epochs, batch_size, num_classes, dir_report, num_channels, path, name_dataset):
    # Load images and Labels
    image_train, label_train = load_patches(path)
    data_train = SegmentationData(image_train, label_train)
    # Setting up the train dataset
    dataset_train = DataLoader(data_train, batch_size=batch_size, shuffle=True, num_workers=0)

    # Model call
    unet = unet_model(num_channel=num_channels, num_classes=num_classes)
    unet.train()

    # Create the experiment directory if not present
    dir_report = Path(dir_report)
    if not dir_report.exists():
        dir_report.mkdir()

    # Specify the loss function
    criterion = torch.nn.CrossEntropyLoss(ignore_index=0)

    # Specify the optimizer with a lower learning rate
    optimizer = torch.optim.Adam(unet.parameters(), lr=0.01)

    # Specify the evaluation metrics
    metrics = {'accuracy': accuracy_score, 'f1_score_weighted': f1_score,
               'balanced_accuracy': balanced_accuracy_score, 'iou': compute_iou}

    # Train
    train_model(model=unet, criterion=criterion,
                dataloaders=dataset_train,
                optimizer=optimizer, metrics=metrics, save_path=dir_report,
                num_epochs=epochs, name_dataset=name_dataset)

    # Inference

    torch.cuda.empty_cache()


# ------------- Applying -------------
cerradatav3 = ['/scratch/ideeps/mateus.miranda/ai4luc/cerradata80k_splited/train2',
             '/scratch/ideeps/mateus.miranda/ai4luc/cerradata80k_splited/ai4luc_masks/mask_train']

# Training CBERS4A RGB
if __name__ == '__main__':
    main(epochs=EPOCHS, batch_size=BATCH_SIZE, num_classes=NUM_CLASSES, path=cerradatav3,
         dir_report=exp_directory, num_channels=3, name_dataset='cerradatav3')
